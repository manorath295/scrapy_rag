from langchain_community.document_loaders import JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI


def setup_and_run_rag_pipeline(question: str):
    """
    Set up and run the RAG (Retrieval-Augmented Generation) pipeline.
    :param question: The input question to ask the model.
    :return: The answer generated by the model.
    """

    # Load documents from output.json
    # loader = JSONLoader(
    #     file_path='output.json',
    #     jq_schema='.[].content',
    #     text_content=False
    # )
    # docs = loader.load()

    # # Split documents into chunks
    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    # splits = text_splitter.split_documents(docs)

    # Initialize embeddings
    embedder = GoogleGenerativeAIEmbeddings(
        google_api_key="AIzaSyCnDPypbbYubsbpWC0k5F8M8Vbkya-RdvY",
        model="models/text-embedding-004"
    )

    # Store in Qdrant vector DB
    # vector_store = QdrantVectorStore.from_documents(
    #     documents=splits,
    #     url="http://localhost:6333",  # Qdrant should be running here
    #     collection_name="learning_langchain5",
    #     embedding=embedder,
    # )

    # Use retriever from existing Qdrant collection
    # retriever = QdrantVectorStore.from_existing_collection(
    #     url="http://localhost:6333",
    #     collection_name="learning_langchain5",
    #     embedding=embedder,
    # )
    # retriever = vector_store.as_retriever()
    retriever = QdrantVectorStore.from_existing_collection(
        url="http://localhost:6333",
        collection_name="learning_langchain5",
        embedding=embedder,
    ).as_retriever()
    # Define the system prompt
    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you don't know.\n\n{context}"
        'you will be working in following way'
        "1-plan 2-think 3-output"
        "after each step ask from user to move further"
    )

    # Define the chat prompt template
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    # Define the LLM for response generation
    llm = ChatGoogleGenerativeAI(
        google_api_key="AIzaSyCnDPypbbYubsbpWC0k5F8M8Vbkya-RdvY",
        model="gemini-2.0-flash"
    )

    # Chain definition
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    # Run the RAG pipeline
    results = rag_chain.invoke({"input": question})
    return results['answer']
